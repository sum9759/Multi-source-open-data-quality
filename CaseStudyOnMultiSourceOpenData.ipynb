{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 4 Datasets\n",
    "# Here, I have taken 2 csv's and one xml document to be merged together. The csv's taken are for dlr_pitch data and \n",
    "# dcc_pitch data. Fingal data has been downloaded first in xml form, then I parsed it to a pandas dataframe using xml.etree \n",
    "# as my parser. Before starting off, we import pandas and numpy since they get used extensively throughout the code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# reading in dlr_pitch data\n",
    "dlr_pitch = pd.read_csv('loading the csv file of dlr_pitch')\n",
    "print (dlr_pitch)\n",
    "\n",
    "# reading in dcc_pitch data\n",
    "dcc_pitch = pd.read_csv('loading the other csv of dcc_pitch data')\n",
    "print(dcc_pitch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dlr_pitch dataset, the location column has sparsely populated values. The Location value is just stated once\n",
    "## Fill all rows of blank location with previous row values\n",
    "dlr_pitch['Location'] = dlr_pitch['Location'].fillna(method='ffill')\n",
    "print(dlr_pitch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the fingal dataset in xml format and converting to a usable pandas dataframe\n",
    "\n",
    "import xml.etree.cElementTree as et\n",
    "xml_file = et.parse(\"load the xml file of fcc playing pitches\")\n",
    "\n",
    "def findval(node):\n",
    "    if node is not None:\n",
    "       return node.text\n",
    "    else:\n",
    "       return None\n",
    "\n",
    "def parsing():\n",
    "    root = xml_file.getroot()\n",
    "    column_name = ['Pitch_type','Name','loc','lat','long']\n",
    "    fcc_data = pd.DataFrame(columns = column_name)\n",
    "\n",
    "    \n",
    "    for node in root.findall('./Playing_Pitches-table/Playing_Pitches'):\n",
    "        Pitch_type = node.find('FACILITY_TYPE')\n",
    "        Name = node.find('FACILITY_NAME')\n",
    "        loc = node.find('LOCATION')\n",
    "        lat = node.find('LAT')\n",
    "        long = node.find('LONG')\n",
    "        \n",
    "        fcc_data = fcc_data.append(\n",
    "            pd.Series([findval(Pitch_type),findval(Name),findval(loc),\n",
    "                        findval(lat),findval(long)],index = column_name),ignore_index=True)\n",
    "    part_data = pd.DataFrame(fcc_data).copy(deep = True)\n",
    "    return part_data\n",
    "    \n",
    "fcc_data = parsing()\n",
    "print(fcc_data)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in dlr, dcc and fcc data\n",
    "dlr_pitch\n",
    "dlr_pitch = dlr_pitch.drop_duplicates(subset=list(dlr_pitch), keep=False)\n",
    "#dlr_pitch\n",
    "print(len(dlr_pitch))\n",
    "\n",
    "## removing duplicates from dcc\n",
    "#len(dcc_pitch)\n",
    "dcc = dcc_pitch.drop_duplicates(subset=list(dcc_pitch), keep=False)\n",
    "print(len(dcc))\n",
    "\n",
    "## fcc\n",
    "print(len(fcc_data))\n",
    "fcc = fcc_data.drop_duplicates(subset=list(fcc_data), keep=False)\n",
    "# fcc\n",
    "## first changing the col names in fcc to latitude and longitude instead of lat, long\n",
    "list(fcc)\n",
    "fcc.columns = ['TypeOfPitch','Name','Location_fcc','Latitude','Longitude']\n",
    "# fcc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking lengths of dlr_pitch, fcc data \n",
    "list(dlr_pitch)\n",
    "list(fcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the dtypes of the two tables\n",
    "print (fcc['Latitude'].dtypes)\n",
    "print (fcc['Longitude'].dtypes)\n",
    "print (dlr_pitch['Latitude'].dtypes)\n",
    "print (dlr_pitch['Longitude'].dtypes)\n",
    "\n",
    "# we see that in fcc - the data type of latitude and longitude is object whereas in dlr_pitch it is float64\n",
    "# thus we convert both to float64\n",
    "fcc['Latitude'] = fcc['Latitude'].astype(np.float64)\n",
    "fcc['Longitude'] = fcc['Longitude'].astype(np.float64)\n",
    "dlr_pitch['Longitude'] = dlr_pitch['Longitude'].astype(np.float64)\n",
    "dlr_pitch['Latitude'] = dlr_pitch['Latitude'].astype(np.float64)\n",
    "\n",
    "# Now we sanity check the fields\n",
    "print (fcc['Latitude'].dtypes)\n",
    "print (fcc['Longitude'].dtypes)\n",
    "print (dlr_pitch['Latitude'].dtypes)\n",
    "print (dlr_pitch['Longitude'].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlr_pitch = dlr_pitch.round({'Latitude': 5, 'Longitude': 5})\n",
    "dlr_pitch['Lat-Long'] = list(zip(dlr_pitch.Latitude, dlr_pitch.Longitude))\n",
    "print(dlr_pitch.head(n=10))\n",
    "\n",
    "fcc = fcc.round({'Latitude': 5, 'Longitude': 5})\n",
    "fcc['Lat-Long'] = list(zip(fcc.Latitude, fcc.Longitude))\n",
    "print(fcc.head(n=10))\n",
    "\n",
    "## merge dlr and fcc pitch data on lat and long\n",
    "dlr_fcc = dlr_pitch.merge(fcc, on=['Lat-Long'], how ='outer')\n",
    "print(dlr_fcc.head(n=6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns of merged dataset to recognize which tables have given the columns\n",
    "dlr_fcc = dlr_fcc.rename(columns = {'Location_x' : 'Loc_dlr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking dcc_pitch'PARK','AREA','CLUBNAME','LEAGUE'\n",
    "dcc_pitch = dcc_pitch.loc[:,['PARK','AREA','CLUBNAME','LEAGUE']]\n",
    "dcc_pitch\n",
    "\n",
    "base_data = pd.read_csv('# loading a dataset with information on all town lat-long ##')\n",
    "base_data\n",
    "\n",
    "## filtered and extracted only dublin data\n",
    "base_data = base_data.loc[base_data['County'] == 'DUBLIN']\n",
    "base_data\n",
    "\n",
    "## select imp columns and drop the rest\n",
    "base_data = base_data.loc[:,['X','Y','County','Contae','Local_Government_Area','English_Name','Irish_Name','Validated_By']]\n",
    "\n",
    "## rename x and y in base_data to Longitude and Latitude \n",
    "base_data = base_data.rename(columns = {'Y':'Latitude', 'X': 'Longitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## on finding suitable merging column from base_data for dcc\n",
    "\n",
    "base_data.head(n=5)\n",
    "base_data.dtypes\n",
    "dcc_pitch.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting column English_name to Location in base_data for easy merging\n",
    "dcc_pitch.head(n=5)\n",
    "base_data.head(n=6)\n",
    "base_data['Location'] = base_data['English_Name'].str.lower()\n",
    "dcc_pitch['Location'] = dcc_pitch['PARK'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging dcc and base_data\n",
    "base_data.head(n=6)\n",
    "dcc_pitch.head(n=5)\n",
    "dcc_final = pd.merge(base_data,dcc_pitch, on = 'Location', how = 'right')\n",
    "dcc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity Check and checking length of data\n",
    "print(len(dcc_final))\n",
    "print(len(dlr_pitch))\n",
    "len(fcc) \n",
    "\n",
    "print (fcc['Latitude'].dtypes)\n",
    "print (fcc['Longitude'].dtypes)\n",
    "print (dcc_final['Latitude'].dtypes)\n",
    "print (dcc_final['Longitude'].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging all datasets together\n",
    "dcc_fcc = pd.merge(dcc_final, fcc, on = ['Latitude','Longitude'], how = 'outer')\n",
    "\n",
    "dcc_fcc_dlr = pd.merge(dcc_fcc,dlr_pitch, on = ['Latitude','Longitude'], how = 'outer' )\n",
    "dcc_fcc_dlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = dcc_fcc_dlr[dcc_fcc_dlr['Latitude'].notnull()]\n",
    "merged_data.head(n=2)\n",
    "\n",
    "keep the important columns from the merged dataset\n",
    "dcc_fcc_dlr = dcc_fcc_dlr.loc[:,['Longitude','Latitude','County','Contae','PARK','AREA',\n",
    "                       'CLUBNAME','LEAGUE','TypeOfPitch','Name','Location_fcc',\n",
    "                       'Number','Size','Local_Government_Area','English_Name','Irish_Name','Validated_By']]\n",
    "dcc_fcc_dlr = dcc_fcc_dlr.drop_duplicates(subset=list(dcc_fcc_dlr), keep=False)\n",
    "\n",
    "# fill all rows of county with dublin since all the data is for pitches around dublin\n",
    "dcc_fcc_dlr['County'] = dcc_fcc_dlr['County'].fillna(method='ffill')\n",
    "dcc_fcc_dlr['Contae'] = dcc_fcc_dlr['Contae'].fillna(method='ffill')\n",
    "dcc_fcc_dlr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fields such as park, location_fcc, name to generate one location field:\n",
    "# First, I make all entries in these columns to lowercase\n",
    "dcc_fcc_dlr = dcc_fcc_dlr.drop('PARK', 1)\n",
    "dcc_fcc_dlr['Park'] = dcc_fcc_dlr['PARK'].str.lower()\n",
    "dcc_fcc_dlr['Location_fcc'] = dcc_fcc_dlr['Location_fcc'].str.lower()\n",
    "dcc_fcc_dlr['English_Name'] = dcc_fcc_dlr['English_Name'].str.lower()\n",
    "dcc_fcc_dlr['Name'] = dcc_fcc_dlr['Name'].str.lower()\n",
    "dcc_fcc_dlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data[merged_data['Location_fcc'].notnull()]\n",
    "## Location_fcc seems to be the superset of name since name is a specific area in location_fcc\n",
    "#Thus I concatenate both into one field\n",
    "dcc_fcc_dlr['Location'] = dcc_fcc_dlr['Name'].map(str)+','+dcc_fcc_dlr['Location_fcc'].map(str)\n",
    "dcc_fcc_dlr[dcc_fcc_dlr['Location_fcc'].notnull()]\n",
    "\n",
    "# ## Combine Park and Location into one column\n",
    "dcc_fcc_dlr['ve'] = dcc_fcc_dlr.Park.fillna(dcc_fcc_dlr.'Location', inplace=True)\n",
    "dcc_fcc_dlr['Park'] = dcc_fcc_dlr['Park'].fillna(dcc_fcc_dlr['Location'])\n",
    "dcc_fcc_dlr\n",
    "dcc_fcc_dlr.drop(['English_Name', 'Loc', 've', 'Irish_Name','Location_fcc','Name'], axis=1, inplace=True)\n",
    "dcc_fcc_dlr.drop(['Location'],axis=1, inplace=True)\n",
    "dcc_fcc_dlr = dcc_fcc_dlr.rename(columns = {'Park': 'Location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcc_fcc_dlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we shall divide the dataset into two- one with no NAs in Latitude and Longitude and one with NAs\n",
    "merged_data = dcc_fcc_dlr[dcc_fcc_dlr['Latitude'].notnull()]\n",
    "lat_long_discrepant = dcc_fcc_dlr[dcc_fcc_dlr['Latitude'].isnull()]\n",
    "lat_long_discrepant\n",
    "# merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting lat long data from google api for location data\n",
    "\n",
    "latitude = []\n",
    "longitude = []\n",
    "import requests\n",
    "## requested an api key from google for easy access to lat long data\n",
    "myapi_key = \"## provide your api key ##\"\n",
    "\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "\n",
    "## using iterrows() for all rows of lat_long_discrepant, we extract each datafield of location value and find its coordinates\n",
    "for i in lat_long_discrepant.iterrows():\n",
    "    ## Searching for coordinates only in Ireland since some places such as Belcamp exist in USA too\n",
    "    resp_lodger = requests.get('https://maps.googleapis.com/maps/api/geocode/json?address={0}&key={1}'.format(i[1][12] + \", Ireland\", myapi_key))\n",
    "    ## Logging all data to json format\n",
    "    respons_json = resp_lodger.json()\n",
    "    ## For all ok status - that is status i have access to\n",
    "    if respons_json['status'] == 'OK':\n",
    "           latitude.append(respons_json['results'][0]['geometry']['location']['lat'])\n",
    "           longitude.append(respons_json['results'][0]['geometry']['location']['lng'])\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting values of the lists latitude and longitude into the respective columns\n",
    "\n",
    "lat_long_discrepant['Latitude'] = latitude\n",
    "lat_long_discrepant['Longitude'] = longitude\n",
    "lat_long_discrepant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally append the two datasets merged_data and new lat_long_discrepant datasets\n",
    "final_dataset = merged_data.append(lat_long_discrepant)\n",
    "final_dataset\n",
    "\n",
    "## remove duplicates\n",
    "final_dataset = final_dataset.drop_duplicates(subset=list(final_dataset), keep=False)\n",
    "final_dataset\n",
    "\n",
    "\n",
    "### output the final dataset as a csv\n",
    "\n",
    "final_dataset.to_csv('##saving the final output as a csv##', sep=',', encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
